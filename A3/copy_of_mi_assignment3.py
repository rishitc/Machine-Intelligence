# -*- coding: utf-8 -*-
"""Copy of MI_Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/104JTLisW8ZaDJEfrqXIl56YQu9mGfpPw
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split


# from google.colab import files
# uploaded = files.upload()

df = pd.read_csv('LBW_Dataset.csv')

df

number_of_null_values_per_col = df.isnull().sum(axis=0)

number_of_null_values_per_col

# look at the # of missing points in the second, third, fourth and fifth columns
number_of_null_values_per_col[1:5]

"""## How many cells of data points are missing?"""

total_no_of_cells = np.product(df.shape)
total_no_of_missing_cells = number_of_null_values_per_col.sum()

percent_missing_cells = (total_no_of_missing_cells / total_no_of_cells)

print(f"Percentage of data cells that is missing is: {percent_missing_cells:.4%}")

"""## Which column has the most number of missing values?"""

print(number_of_null_values_per_col.idxmax(), number_of_null_values_per_col.max())

#Analyse each column and checkout diff things from here

#Education - All 5 - Scam
df["Education"].fillna(5, inplace=True)
df["Education"].unique()

#Residence - use mode to replace it
df["Residence"].fillna(df.Residence.mode()[0], inplace=True)
df["Residence"].unique()

#Delivery Phase - use mode to replace
df["Delivery phase"].fillna(df["Delivery phase"].mode()[0], inplace=True)
df["Delivery phase"].unique()

df.groupby('Result').transform('mean')

#Use Mean-By-Output-Class for "Weight, Age, HB, BP"
df = df.fillna(df.groupby('Result').transform('mean'))

#Check Weight
df["Weight"].unique()

#Check Age
df["Age"].unique()

#Cheeck HB
df["HB"].unique()

#Check BP
df["BP"].unique()

df.isnull().sum(axis = 0)

def get_entropy_of_dataset(df):
    # gets the target column
    target = df.loc[:, df.columns[-1]]
    unique_values = target.unique().tolist()

    # number of unique values
    uniq = len(unique_values)

    target = target.tolist()
    vals = []
    for i in range(uniq):
        vals.append(target.count(unique_values[i]))
    summ = sum(vals)
    entropy = 0
    for i in range(uniq):
        entropy += (-1)*(vals[i]/summ)*(np.log2(vals[i]/summ))
    return entropy

def entropyFormula(answerDict):
    valueOfAttribute_entropy = {}

    for value in answerDict:
        temp = answerDict[value].values()
        denominator = sum(temp)
        entropy = 0

        for count in temp:
            if (count != 0):
                entropy += -(count/denominator) * np.log2(count/denominator)

        valueOfAttribute_entropy[value] = entropy

    return valueOfAttribute_entropy

def avgInformationEntropy(answerDict, valueOfAttribute_entropy,
                          totalNumberOfSamples):
    answer = 0
    for valueOfAttribute in answerDict:
        numerator = sum(answerDict[valueOfAttribute].values())
        answer += numerator / totalNumberOfSamples * \
            valueOfAttribute_entropy[valueOfAttribute]

    return answer

def get_entropy_of_attribute(df, attribute):
    entropy_of_attribute = 0

    if attribute not in df.columns.tolist():
        return entropy_of_attribute

    # Get the name of the target attribute column
    TARGET_ATTRIBUTE = df.columns.tolist()[-1]

    # Get the list of unique values the attribute can take
    valuesOfAttribute = df[attribute].unique().tolist()

    # Get the list of unique values the TARGET_ATTRIBUTE can take
    valuesOfTargetAttribute = df[TARGET_ATTRIBUTE].unique().tolist()

    # Create a dictionary containing the values of attribute as keys
    # and the values as the outcome:no_of_occurrences
    # E.g.:
    # {
    # 	'val1': {'a':1, 'b':0},
    # 	'val2': {'a':0, 'b':2}
    # }
    answerDict = {}
    for value in valuesOfAttribute:
        answerDict[value] = dict.fromkeys(valuesOfTargetAttribute, 0)

    # print(answerDict)

    # Create a dictionary to hold the unique attribute value with the
    # corresponding df as key-value pairs
    valuesOfAttribute_Dataframe = {}

    # Fill in the values in to the: valuesOfAttribute_Dataframe, dictionary
    for value in valuesOfAttribute:
        valuesOfAttribute_Dataframe[value] = df.loc[df[attribute] == value]

    # For each dataframe
    for valueOfAttribute in valuesOfAttribute_Dataframe:
        dataframe = valuesOfAttribute_Dataframe[valueOfAttribute]

        for valueOfTargetAttribute in valuesOfTargetAttribute:
            # Find to the count of different entries in the target attribute
            temp = dataframe.loc[dataframe[TARGET_ATTRIBUTE] ==
                                 valueOfTargetAttribute] \
                .count().tolist()[-1]

            answerDict[valueOfAttribute][valueOfTargetAttribute] = temp

    # print(answerDict)

    # Use the entropy formula and get the entropy of all the attribute
    # value pairs and take the sum
    valueOfAttribute_entropy = entropyFormula(answerDict)
    entropy_of_attribute = avgInformationEntropy(answerDict,
                                                 valueOfAttribute_entropy,
                                                 df.shape[0])

    # 3. Return the sum
    return abs(entropy_of_attribute)

def get_information_gain(df, attribute):
    return abs(get_entropy_of_dataset(df) - get_entropy_of_attribute(df, attribute))

#Find IG for each column
information_gains = dict()
for i in df.columns.tolist():
  information_gains[i] = get_information_gain(df, i)

information_gains

max_IG = max(information_gains.values())
for i in df.columns.tolist():
  if(information_gains[i] < 0.3*max_IG):
    print(f"{i} column is dropped")
    del df[i]

df

cols = df.columns.to_list()

# Create training, validation and test splits
df_train = df.sample(frac=0.8, random_state=42)
df_test = df.drop(df_train.index)

df_valid = df_train.sample(frac=0.15, random_state=101)

df_train = df_train.drop(df_valid.index)

print(set(df_train.index).intersection(df_test.index))

df_train.reset_index(drop=True, inplace=True)
df_valid.reset_index(drop=True, inplace=True)
df_test.reset_index(drop=True, inplace=True)

# Sanity check to make sure that there is no row that is in more than one dataframe
df_train.merge(df_valid, how='inner', on=df.columns.to_list())

# Sanity check to make sure that there is no row that is in more than one dataframe
df_train.merge(df_test, how='inner', on=df.columns.to_list())

df.loc[(df["Weight"] == 43.0) & (df["HB"] == 9.2) & (df["BP"] == 1.375)]

df_test.loc[df_test["Weight"] == 43.0]

df_train.loc[df_train["Weight"] == 43.0]

# Sanity check to make sure that there is no row that is in more than one dataframe
df_test.merge(df_valid, how='inner', on=df.columns.to_list())

df_train.head()

# Scale to [0, 1]
max_ = df_train.max(axis=0)
min_ = df_train.min(axis=0)
df_train = (df_train - min_) / (max_ - min_)
df_valid = (df_valid - min_) / (max_ - min_)
df_test = (df_test - min_) / (max_ - min_)

# One-hot encoding as we are using softmax activation function
df_train = pd.get_dummies(df_train, prefix='Result', columns=['Result'])
df_test = pd.get_dummies(df_test, prefix='Result', columns=['Result'])
df_valid = pd.get_dummies(df_valid, prefix='Result', columns=['Result'])

df_train

df_train.loc[df_train["Result_0.0"] == 1]

df_valid

df_test

df_train.to_csv("train_split.csv", index=False)
df_valid.to_csv("valid_split.csv", index=False)
df_test.to_csv("test_split.csv", index=False)