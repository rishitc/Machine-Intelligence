{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Genetic_NeuralNetwork_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7OB9JwvYLbA"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"NeuralNetwork.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1MBod6QEwuUU4McbyAahCaYDWjkP7SA-A\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "import pandas as pd\n",
        "import sys\n",
        "import random\n",
        "import copy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHtN8jQxYRMx"
      },
      "source": [
        "class Layer:\n",
        "    def __init__(self, _numInputs, _numNeurons, _activFunc):\n",
        "        # Note, numInputs = number of neurons in the previous layer\n",
        "        self.activFuncName = _activFunc\n",
        "        self.prevShape = _numInputs + 1  # For bias\n",
        "        self.shape = _numNeurons\n",
        "        self.droprate = 0.1  # The proportion of neurons to drop at a layer\n",
        "        self.seed = np.random.RandomState(42)\n",
        "        sd = np.sqrt(6.0 / (self.prevShape + self.shape))\n",
        "        self.weights = np.random.uniform(-sd, sd, (self.prevShape, self.shape))\n",
        "\n",
        "        # Setting the bias values to 0 in the weights matrix\n",
        "        for i in range(self.shape):\n",
        "            self.weights[-1][i] = 0\n",
        "\n",
        "        \"\"\" Returns the shape of the weights matrix in that layer (object)\n",
        "        \"\"\"\n",
        "    def shape(self):\n",
        "        return self.weights.shape\n",
        "\n",
        "    @classmethod\n",
        "    def ReLU(cls, inputs):\n",
        "        return np.maximum(inputs, np.zeros(inputs.shape))\n",
        "\n",
        "        # matrix = np.transpose(inputs)\n",
        "        # return np.transpose(np.array([\n",
        "        #                               [max(0.0, x) for x in matrix[j]]\n",
        "        #                               for j in range(len(matrix))\n",
        "        #                               ])\n",
        "        #                     )\n",
        "\n",
        "    @classmethod\n",
        "    def ReLU_Prime(cls, inputs):\n",
        "        assert inputs.shape[1] == 65, \\\n",
        "            \"The 65 rows of the dataset should be along the \\\n",
        "                columns of the input matrix\"\n",
        "        return np.maximum(inputs, np.zeros(inputs.shape))\n",
        "        \"\"\"\n",
        "        We take the transpose so that the output for a particular input row\n",
        "        from the dataset is now along the rows of the matrix\n",
        "        \"\"\"\n",
        "        # matrix = np.transpose(inputs)\n",
        "\n",
        "        \"\"\"\n",
        "        We take the transpose so that the output for a particular\n",
        "        input row from the dataset is now along the cols of the matrix\n",
        "        \"\"\"\n",
        "        # return np.transpose(np.array([\n",
        "        #                               [1 if i > 0 else 0 for i in matrix[j]]\n",
        "        #                               for j in range(len(matrix))\n",
        "        #                               ])\n",
        "        #                     )\n",
        "\n",
        "    @classmethod\n",
        "    def softmax(cls, inputs):\n",
        "        assert inputs.shape[1] == 65, \"The 65 rows of the dataset should be along the columns of the input matrix\"\n",
        "        # After this transpose, every output for a particular\n",
        "        # input row from the dataset, will be placed row wise\n",
        "        matrix = np.transpose(inputs)\n",
        "        ret = []\n",
        "\n",
        "        res = (matrix.T - np.max(matrix, axis=1)).T\n",
        "        res = np.exp(res)\n",
        "\n",
        "        ans = res.T / np.sum(res, axis=1)\n",
        "        assert ans.shape == (2, 65), f\"The dimensions of {ans} are: {ans.shape}\"\n",
        "        # This step of transpose is needed so that every output for\n",
        "        # a particular input row to the model, is now column-wise\n",
        "        return ans\n",
        "\n",
        "        # The code below has been decommissioned\n",
        "        # for row in matrix:\n",
        "        #     exps = [np.exp(x) for x in row]\n",
        "        #     sumexps = sum(exps)\n",
        "        #     ret.append(np.array([exps[i] / sumexps for i in range(len(exps))]))\n",
        "\n",
        "        # ret = np.array(ret)\n",
        "        # assert type(ret) == np.ndarray\n",
        "\n",
        "        # # This step of transpose is needed so that every output for\n",
        "        # # a particular input row to the model, is now column-wise\n",
        "        # return np.transpose(ret)\n",
        "\n",
        "    @classmethod\n",
        "    def softmax_Prime(cls, inputs):\n",
        "        assert inputs.shape[1] == 65, \\\n",
        "            \"The 65 rows of the dataset should be \\\n",
        "                along the columns of the input matrix\"\n",
        "        '''\n",
        "            d(S(Zi))/dZj = derivatives[i][j]\n",
        "        '''\n",
        "        # After this transpose, every output for a particular\n",
        "        # input row from the dataset, will be placed row wise\n",
        "        matrix = np.transpose(inputs)\n",
        "        ret = []\n",
        "        for row in matrix:\n",
        "            exps = np.exp(row)  # [np.exp(x) for x in row]\n",
        "            sum_exps = np.sum(exps)\n",
        "            derivatives = [[(exps[i]/sum_exps) * (1 - (exps[i]/sum_exps))\n",
        "                            if i == j else (exps[i]/sum_exps) *\n",
        "                            -1 * (exps[j]/sum_exps)\n",
        "                            for j in range(len(row))] for i in range(len(row))]\n",
        "            ret.append(np.array(derivatives))\n",
        "        assert type(ret) == list\n",
        "        return np.transpose(np.array(ret))\n",
        "\n",
        "    def set_params(self, _weights):  # , _biases):\n",
        "        temp_weights = self.weights\n",
        "        self.weights = _weights\n",
        "        return temp_weights\n",
        "\n",
        "    def get_params(self):\n",
        "        return self.weights\n",
        "\n",
        "    def drop(self):\n",
        "        return np.random.binomial(1, 1 - self.droprate, size=self.shape)\n",
        "\n",
        "    def forward(self, _input, _train=True):\n",
        "        # Here we perform the matrix multiplication of W^T * X\n",
        "        self.output = np.dot(self.weights.T, _input)\n",
        "        if _train:\n",
        "            self.activeNeurons = self.drop()\n",
        "            temp1 = self.activeNeurons.copy()\n",
        "            temp = self.output.shape[1]\n",
        "            for i in range(temp - 1):\n",
        "                self.activeNeurons = np.vstack((self.activeNeurons, temp1))\n",
        "            self.activeNeurons = self.activeNeurons.transpose()\n",
        "            # Performs elements wise multiplication\n",
        "            self.output *= self.activeNeurons\n",
        "\n",
        "            # Divide the output matrix by the fraction of outputs\n",
        "            # kept and not dropped\n",
        "            # We perform elements wise division here\n",
        "            self.output = self.output/(np.count_nonzero(self.activeNeurons) /\n",
        "                                       np.size(self.activeNeurons))\n",
        "        return self.activationFunc(self.activFuncName, self.output)\n",
        "\n",
        "    def activationFunc(self, _activFuncName, inputs):\n",
        "        if(_activFuncName == 'ReLU'):\n",
        "            return self.ReLU(inputs)\n",
        "        elif(_activFuncName == 'softmax'):\n",
        "            return self.softmax(inputs)\n",
        "        else:\n",
        "            # Exit the program on failure\n",
        "            print(\"Wrong Activation Function Name!\")\n",
        "            sys.exit(1)\n",
        "\n",
        "    def backward(self, _currentLayerDelta, _prevLayerOutputs):\n",
        "        \"\"\"\n",
        "            Computes delta values for previous layer\n",
        "        \"\"\"\n",
        "        # If the previous layer is a hidden layer\n",
        "        prevlayer = [0 for i in range(_prevLayerOutputs.size)]\n",
        "        for i in range(_prevLayerOutputs.size):\n",
        "            if (_prevLayerOutputs[i] > 0):\n",
        "                prevlayer[i] = 1\n",
        "            else:\n",
        "                prevlayer[i] = 0\n",
        "        prevlayer = np.array(prevlayer)\n",
        "        a = np.dot(self.weights, _currentLayerDelta)\n",
        "        print(prevlayer)\n",
        "        print(a)\n",
        "        return np.multiply(a, prevlayer)\n",
        "\n",
        "# class BackPropagation:\n",
        "#     @classmethod\n",
        "#     def outputLayerBackpropGradient(cls, yHat, y, layer_input, W_matrix):\n",
        "#         # We take the transpose so that the output for\n",
        "#         # each input row from the dataset is now row-wise in yHat\n",
        "#         yHatT = np.transpose(yHat)\n",
        "#         # yT = np.transpose(y)\n",
        "#         ret = []\n",
        "#         # The dimensions must be the same\n",
        "#         assert yHatT.shape == y.shape, f\"Shape of yHatT = {yHatT.shape}\\nShape of y = {y.shape}\"\n",
        "\n",
        "#         W_matrix_T = np.transpose(W_matrix)\n",
        "#         assert W_matrix_T.shape == (2, 7), f\"The actual shape of W_matrix_T is {W_matrix_T.shape}\"\n",
        "\n",
        "#         layer_input_T = np.transpose(layer_input)\n",
        "#         assert layer_input_T.shape[0] == 65, f\"The actual shape of layer_input_T is {layer_input_T.shape}\"\n",
        "        \n",
        "#         weight_gradient_per_output = []\n",
        "\n",
        "#         rowCount, colCount = W_matrix_T.shape\n",
        "#         for truValInd, truVal in enumerate(y):\n",
        "#             weight_gradient = []\n",
        "#             for rowInd in range(rowCount):\n",
        "#                 weight_gradient_row = []\n",
        "#                 for colInd in range(colCount):\n",
        "#                     ans = (yHatT[truValInd][rowInd] - (1 if np.argmax(truVal) == rowInd else 0)) * layer_input_T[truValInd][rowInd]\n",
        "#                     ans = ans * (1 / rowCount)\n",
        "#                     assert type(ans) != np.ndarray\n",
        "#                     weight_gradient_row.append(ans)\n",
        "#                 weight_gradient.append(weight_gradient_row)\n",
        "#             weight_gradient_per_output.append(weight_gradient)\n",
        "        \n",
        "#         weight_gradient_per_output = np.array(weight_gradient_per_output)\n",
        "\n",
        "#         print(weight_gradient_per_output.shape)\n",
        "\n",
        "#         # Get the mean gradient for the output layer\n",
        "#         return np.mean(weight_gradient_per_output, axis = 0)\n",
        "\n",
        "#     @classmethod\n",
        "#     def middleLayerBackpropGradient(cls, yHat, layer_input, W_matrix, outputLayer_W_matrix_wo_bias, output_layer_loss):\n",
        "#         assert outputLayer_W_matrix_wo_bias.shape == output_layer_loss.shape\n",
        "#         _NO_OF_NEURONS = W_matrix.shape[0]\n",
        "\n",
        "#         for i in range(_NO_OF_NEURONS):\n",
        "#             temp = np.reshape(layer_input, shape=(65, 9, 1))\n",
        "\n",
        "#         res = np.ones(shape=W_matrix.shape) * layer_input * Layer.ReLU_Prime(yHat) * np.sum(outputLayer_W_matrix_wo_bias * next_layer_loss, axis=1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ8hzdAoYcd4"
      },
      "source": [
        "class NeuralNet:\n",
        "    _THRESHOLD = 0.5\n",
        "\n",
        "    def __init__(self):\n",
        "        self.hL1 = Layer(3, 8, 'ReLU')\n",
        "        self.hL2 = Layer(8, 6, 'ReLU')\n",
        "        self.outL = Layer(6, 2, 'softmax')\n",
        "        self.layers = [self.hL1, self.hL2, self.outL]\n",
        "        # self.layer_outputs = []\n",
        "\n",
        "    def fit(self, inputs, _train, _numEpochs, truthValues):\n",
        "        for i in range(_numEpochs):\n",
        "            output = inputs\n",
        "            # self.layer_outputs.append(output)\n",
        "            for layer, to_train in zip(self.layers, _train):\n",
        "                # np.array([1 for i in range(output.shape[1])])))\n",
        "                output = np.vstack((output, np.ones(output.shape[1])))\n",
        "                # self.layer_outputs.append(output)\n",
        "                output = layer.forward(output, to_train)\n",
        "\n",
        "            # self.layer_outputs.append(output)\n",
        "            # self.layer_outputs.append(np.vstack((output,\n",
        "            # np.ones(output.shape[1]))))\n",
        "\n",
        "            epoch_loss = self.loss(output, truthValues)\n",
        "            epoch_accuracy = self.accuracy(output, truthValues)\n",
        "            epoch_loss = np.mean(epoch_loss)\n",
        "            # print()\n",
        "            # print(f\"> Epoch: {i} --> Loss: \\n{epoch_loss}, \\\n",
        "            #       Accuracy: {epoch_accuracy}\")\n",
        "            return epoch_accuracy, epoch_loss\n",
        "            # Backpropagation takes place here...\n",
        "            # print(BackPropagation.outputLayerBackpropGradient(\n",
        "            # self.layer_outputs[-1], truthValues, self.layer_outputs[-2],\n",
        "            # self.layers[-1].weights))\n",
        "            # print(BackPropagation.middleLayerBackpropGradient(\n",
        "            # self.layer_outputs[-2], self.layer_outputs[-3],\n",
        "            # self.layers[-2].weights[:-1, :], ))\n",
        "\n",
        "    @classmethod\n",
        "    def loss(self, yHat, y):\n",
        "        # We take the transpose so that the output for\n",
        "        # each input row from the dataset is now row-wise in yHat\n",
        "        yHatT = np.transpose(yHat)\n",
        "        # yT = np.transpose(y)\n",
        "        ret = []\n",
        "        # The dimensions must be the same\n",
        "        assert yHatT.shape == y.shape, \\\n",
        "            f\"Shape of yHatT = {yHatT.shape}\\nShape of y = {y.shape}\"\n",
        "\n",
        "        # Going row-wise, i.e. corresponding input and output-wise\n",
        "        for rowYHAT, rowY in zip(yHatT, y):  # yT):\n",
        "            # They need to be of the same length as if there\n",
        "            # are 2 target values then we need 2 outputs, per\n",
        "            # row\n",
        "            assert len(rowYHAT) == len(rowY)\n",
        "            # print(\"rowYHAT is: \", rowYHAT)\n",
        "            # print(\"rowY is: \", rowY)\n",
        "            ret.append(\n",
        "                        -1 * sum(\n",
        "                                 np.array(\n",
        "                                          [\n",
        "                                           np.log(rowYHAT[i] + 10e-8)\n",
        "                                           # 10e-8 is needed to prevent nan\n",
        "                                           # values\n",
        "                                           if rowY[i] > 0  # == 1\n",
        "                                           else 0  # np.log(1 - rowYHAT[i])\n",
        "                                           for i in range(len(rowYHAT))\n",
        "                                           ]\n",
        "                                          )\n",
        "                                ) / len(rowY)\n",
        "                        )\n",
        "        # print(\"ret is: \\n\", ret)\n",
        "        return np.array(ret)\n",
        "\n",
        "    @classmethod\n",
        "    def loss_prime(self, yHat, y):\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def threshold_func(cls, x):\n",
        "        return 0 if x <= cls._THRESHOLD else 1\n",
        "\n",
        "    def accuracy(self, yHat, y) -> float:\n",
        "        # We take the transpose so that the output for\n",
        "        # each input row from the dataset is now row-wise in yHat\n",
        "        yHatT = np.transpose(yHat)\n",
        "        # yT = np.transpose(y)\n",
        "        # ret = []\n",
        "        # The dimensions must be the same\n",
        "        assert yHatT.shape == y.shape\n",
        "\n",
        "        # Count for the number of correctly classified training samples\n",
        "        correctly_classified_count = 0\n",
        "\n",
        "        # Going row-wise, i.e. corresponding input and output-wise\n",
        "        for rowYHAT, rowY in zip(yHatT, y):  # yT):\n",
        "            # They need to be of the same length as if there\n",
        "            # are 2 target values then we need 2 outputs per\n",
        "            # row\n",
        "            assert len(rowYHAT) == len(rowY)\n",
        "            # print(rowYHAT, rowY)\n",
        "\n",
        "            rowYHAT_after_threshold = rowYHAT.astype(int)\n",
        "\n",
        "            for i in range(len(rowYHAT)):\n",
        "                # Apply the threshold on rowYHAT values\n",
        "                rowYHAT_after_threshold[i] = NeuralNet.\\\n",
        "                                                threshold_func(rowYHAT[i])\n",
        "            if all(rowYHAT_after_threshold == rowY):\n",
        "                correctly_classified_count += 1\n",
        "\n",
        "            #     # if True_Positive or False_Positive\n",
        "            #     rowYHAT_after_threshold = [lambda x : 0\n",
        "            #                                if x <= _THRESHOLD else 1]\n",
        "            #     if (rowYHAT[i] >= _THRESHOLD and rowY[i] == 1) \\\n",
        "            #        or (rowYHAT[i] <= _THRESHOLD and rowY[i] == 0):\n",
        "            #         correctly_classified_count += 1\n",
        "\n",
        "        # The number of true values cannot be more than the number of input\n",
        "        # rows\n",
        "        assert correctly_classified_count <= y.shape[0]\n",
        "\n",
        "        # ret.append(correctly_classified_count / y.shape[0])\n",
        "        # return ret\n",
        "\n",
        "        return correctly_classified_count / y.shape[0]\n",
        "\n",
        "# l1 = Layer(3, 5, 'ReLU')\n",
        "# print(l1.weights)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwOkK2KJYm9x"
      },
      "source": [
        "class Chromosome:\n",
        "    neural_net_obj = NeuralNet()\n",
        "\n",
        "    def __init__(self, layer_list) -> None:\n",
        "        self.numberOfNeuronsInNetwork = 0\n",
        "        self.numberOfNeuronsPerLayer = []\n",
        "        self.chromosome = np.array([])\n",
        "\n",
        "        for layer in layer_list:\n",
        "            flattened_weight_matrix = layer.weights.flatten()\n",
        "            self.chromosome = np.hstack((self.chromosome,\n",
        "                                         flattened_weight_matrix))\n",
        "            self.numberOfNeuronsPerLayer.append(layer.weights.shape[1])\n",
        "\n",
        "        assert len(self.numberOfNeuronsPerLayer) == 3\n",
        "\n",
        "        self.numberOfNeuronsInNetwork = sum(self.numberOfNeuronsPerLayer)\n",
        "        assert self.numberOfNeuronsInNetwork == 16, \\\n",
        "               f\"The number of neurons in the neural network is 16, \\\n",
        "                   but you got {self.numberOfNeuronsInNetwork}\"\n",
        "\n",
        "    def _rebuildChromosome(self, selected_weight_matrix, layer):\n",
        "        assert 0 <= layer <= 2, \\\n",
        "               f\"Layer variable value is not in the range [0, 2]\"\n",
        "        if layer == 0:\n",
        "            assert self.chromosome[:32].shape == selected_weight_matrix\\\n",
        "                                                 .flatten().shape\n",
        "            self.chromosome[:32] = selected_weight_matrix.flatten()\n",
        "            assert np.allclose(self.chromosome[:32], selected_weight_matrix.flatten()) is True,\\\n",
        "                    \"The assignment does not seem to be happening\"\n",
        "        elif layer == 1:\n",
        "            assert self.chromosome[32:86].shape == selected_weight_matrix\\\n",
        "                                                   .flatten().shape\n",
        "            self.chromosome[32:86] = selected_weight_matrix.flatten()\n",
        "            assert np.allclose(self.chromosome[32:86], selected_weight_matrix.flatten()) is True,\\\n",
        "                    \"The assignment does not seem to be happening\"\n",
        "        else:\n",
        "            assert self.chromosome[86:].shape == selected_weight_matrix\\\n",
        "                                                 .flatten().shape\n",
        "            self.chromosome[86:] = selected_weight_matrix.flatten()\n",
        "            assert np.allclose(self.chromosome[86:], selected_weight_matrix.flatten()) is True,\\\n",
        "                    \"The assignment does not seem to be happening\"\n",
        "        \n",
        "        # print(f\"New Chromosome: {self.chromosome}\")\n",
        "\n",
        "    def getWeights(self):\n",
        "        weight_matrix_1 = np.reshape(self.chromosome[:32], newshape=(4, 8))\n",
        "        weight_matrix_2 = np.reshape(self.chromosome[32:86], newshape=(9, 6))\n",
        "        weight_matrix_3 = np.reshape(self.chromosome[86:], newshape=(7, 2))\n",
        "\n",
        "        return weight_matrix_1, weight_matrix_2, weight_matrix_3\n",
        "\n",
        "    def mutate(self, n=2):\n",
        "        # Select n non-input nodes from the chromosome\n",
        "        nodes_to_mutate = np.random\\\n",
        "                            .randint(0, self.numberOfNeuronsInNetwork, n)\n",
        "\n",
        "        # Find the weight matrix to which n belongs to and mutate its weights,\n",
        "        # i.e. that particular column\n",
        "        for neuron in nodes_to_mutate:\n",
        "            layerNo = 0  # Start from the first layer\n",
        "            while neuron > self.numberOfNeuronsPerLayer[layerNo]:\n",
        "                neuron -= self.numberOfNeuronsPerLayer[layerNo]\n",
        "                layerNo += 1\n",
        "            selected_weight_matrix = self.getWeights()[layerNo]\n",
        "            assert type(selected_weight_matrix) is np.ndarray, \\\n",
        "                   f\"The type of the selected_weight_matrix is not a numpy\\\n",
        "                        array, but it is {type(selected_weight_matrix)}\"\n",
        "\n",
        "            # A column of the W matrix is the weights and bias of the neuron\n",
        "            input_links_of_selected_neuron = selected_weight_matrix[:,\n",
        "                                                                    neuron - 1]\n",
        "            _old_chromosome = self.chromosome.copy()\n",
        "            # print(f\"Old Chromosome: {self.chromosome}\")\n",
        "\n",
        "            # Mutate by adding a random value from the initialization\n",
        "            # probability distribution\n",
        "            prevShape = selected_weight_matrix.shape[0]\n",
        "            shape = selected_weight_matrix.shape[1]\n",
        "            sd = np.sqrt(6.0 / (prevShape + shape))\n",
        "\n",
        "            # print(f\"{input_links_of_selected_neuron}\")\n",
        "            # print(f\"{selected_weight_matrix}\")\n",
        "            # The mutation happens here\n",
        "            input_links_of_selected_neuron += np.random.uniform(-sd, sd,\n",
        "                                                                input_links_of_selected_neuron.shape)\n",
        "            # print(f\"{input_links_of_selected_neuron}\")\n",
        "\n",
        "            # Assign the mutated incoming links back to the weights matrix\n",
        "            selected_weight_matrix[:, neuron - 1] = input_links_of_selected_neuron\n",
        "            # print(f\"{selected_weight_matrix}\")\n",
        "            # print(f\"Old Chromosome: {self.chromosome}\")\n",
        "            # Add the mutation to the chromosome\n",
        "            self._rebuildChromosome(selected_weight_matrix, layerNo)\n",
        "            # print(f\"New Chromosome: {self.chromosome}\")\n",
        "            assert np.allclose(_old_chromosome, self.chromosome) is False,\\\n",
        "            f\"Mutation did not change anything!\\nOld Chromosome: {_old_chromosome}\\\n",
        "            \\nNew Chromosome: {self.chromosome}\"\n",
        "\n",
        "    def evaluate(self, inputs, truthValues, _train=[True, True, False]):\n",
        "        new_weights = self.getWeights()\n",
        "        layerList = Chromosome.neural_net_obj.layers\n",
        "\n",
        "        # Set the weights of the chromosome to the layer object\n",
        "        for layerInd, layer in enumerate(layerList):\n",
        "            # Change the weight matrix associated with the layer object\n",
        "            layer.set_params(new_weights[layerInd])\n",
        "\n",
        "        self.chromosome_accuracy, self.chromosome_loss = Chromosome\\\n",
        "            .neural_net_obj.fit(inputs, _train, 1, truthValues)\\\n",
        "            # (self, inputs, _train, _numEpochs, truthValues):\n",
        "\n",
        "        return self.chromosome_accuracy, self.chromosome_loss\n",
        "\n",
        "    @classmethod\n",
        "    def cross_over(cls, parent_1, parent_2):\n",
        "        \"\"\"cross_over Generate a new Chromosome object from the 2 input\n",
        "        parent chromosome objects\n",
        "\n",
        "        :param parent_1: First parent for crossover\n",
        "        :type parent_1: Chromosome class\n",
        "        :param parent_2: Second parent for crossover\n",
        "        :type parent_2: Chromosome class\n",
        "        :return: Child object created from the 2 parent objects\n",
        "        :rtype: Chromosome class\n",
        "        \"\"\"\n",
        "        assert parent_1.numberOfNeuronsInNetwork == parent_2\\\n",
        "               .numberOfNeuronsInNetwork, \"The number of neurons in \\\n",
        "                                           both the networks are \\\n",
        "                                           not the same!\"\n",
        "        # Contains the weights matrices for all the layers of the child\n",
        "        # chromosome\n",
        "        child_layer_list = []\n",
        "        for layerInd, neuronsCount in enumerate(parent_1\n",
        "                                                .numberOfNeuronsPerLayer):\n",
        "            childlayer_weight_matrix = np.array([])\n",
        "            for neuronInd in range(neuronsCount):\n",
        "                # Randomly pick between the parent_1 and parent_2\n",
        "                chosen_parent_index = np.random.randint(0, 2, 1)\n",
        "                if chosen_parent_index == 0:  # parent_1 is chosen\n",
        "                    # Get the weights and bias of a particular neuron in\n",
        "                    # parent_1\n",
        "                    incoming_links = parent_1.getWeights()[layerInd]\\\n",
        "                                     .T[neuronInd, :]\n",
        "                    \n",
        "                    if childlayer_weight_matrix.shape == (0,):\n",
        "                        childlayer_weight_matrix = np\\\n",
        "                            .hstack((childlayer_weight_matrix, incoming_links))\n",
        "                    else:\n",
        "                        # print(incoming_links, incoming_links.shape, type(incoming_links))\n",
        "                        # print(childlayer_weight_matrix)\n",
        "                        assert childlayer_weight_matrix.shape != (0,)\n",
        "                        childlayer_weight_matrix = np\\\n",
        "                            .vstack((childlayer_weight_matrix, incoming_links))\n",
        "                else:  # parent_2 is chosen\n",
        "                    # Get the weights and bias of a particular neuron in\n",
        "                    # parent_2\n",
        "                    incoming_links = parent_2.getWeights()[layerInd]\\\n",
        "                        .T[neuronInd, :]\n",
        "\n",
        "                    if childlayer_weight_matrix.shape == (0,):\n",
        "                        childlayer_weight_matrix = np\\\n",
        "                            .hstack((childlayer_weight_matrix, incoming_links))\n",
        "                    else:\n",
        "                        # print(incoming_links, incoming_links.shape, type(incoming_links))\n",
        "                        # print(childlayer_weight_matrix)\n",
        "                        assert childlayer_weight_matrix.shape != (0,)\n",
        "                        childlayer_weight_matrix = np\\\n",
        "                            .vstack((childlayer_weight_matrix, incoming_links))\n",
        "                    # childlayer_weight_matrix = np.vstack((childlayer_weight_matrix, incoming_links))\n",
        "\n",
        "            assert np.transpose(childlayer_weight_matrix).shape[1] in [2, 6, 8],\\\n",
        "                   f\"{np.transpose(childlayer_weight_matrix)} has the shape\\\n",
        "                       {np.transpose(childlayer_weight_matrix).shape}\"\n",
        "            child_layer_list.append(np.transpose(childlayer_weight_matrix)\n",
        "                                    .copy())\n",
        "        assert len(child_layer_list) == 3, f\"The child_layer_list looks like \\\n",
        "                                             this: {child_layer_list}\"\n",
        "\n",
        "        temp = NeuralNet()\n",
        "        for ind, layer in enumerate(temp.layers):\n",
        "            layer.set_params(child_layer_list[ind])\n",
        "\n",
        "        assert np.allclose(temp.layers[0].get_params(), child_layer_list[0]) \\\n",
        "            is True, f\"{temp.layers[0].get_params()} != {child_layer_list[0]}\"\n",
        "        assert np.allclose(temp.layers[1].get_params(), child_layer_list[1]) \\\n",
        "            is True, f\"{temp.layers[1].get_params()} != {child_layer_list[1]}\"\n",
        "        assert np.allclose(temp.layers[2].get_params(), child_layer_list[2]) \\\n",
        "            is True, f\"{temp.layers[2].get_params()} != {child_layer_list[2]}\"\n",
        "        return Chromosome(temp.layers)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gxxCX4xKhcD"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZFvDUhRYFI7"
      },
      "source": [
        "class GeneticAlgo:\n",
        "    _CHROMOSOME_INDEX = 3\n",
        "    _TIE_BREAKING_INDEX = 2\n",
        "    _LOSS_INDEX = 1\n",
        "    _ACCURACY_INDEX = 0\n",
        "    _TRUTH_VALUES = None\n",
        "    _INPUTS = None\n",
        "\n",
        "    def __init__(self, init_population=50, inputs=None, truthValues=None):\n",
        "        assert inputs is not None, \"input not provided!\"\n",
        "        assert truthValues is not None, \"truthValues not provided!\"\n",
        "        self.tournament_size = 0\n",
        "        GeneticAlgo._INPUTS = inputs\n",
        "        GeneticAlgo._TRUTH_VALUES = truthValues\n",
        "        \"\"\"\n",
        "        Creating the initial population, for the genetic algorithm\n",
        "\n",
        "        The format for every member of the population list is:\n",
        "        (<Chromosome_Accuracy>, -1 * abs(<Chromosome_Loss>), <Tie_Breaking_Value>, <Chromosome_Object>)\n",
        "        \"\"\"\n",
        "        # We have created the initial population, so generation-0\n",
        "        self.generation = 0\n",
        "        self.population = []\n",
        "        self.population_count = init_population\n",
        "        for _ in range(init_population):\n",
        "            self.population.append([1, -np.absolute(2), _,\n",
        "                                   Chromosome(NeuralNet().layers)])\n",
        "\n",
        "        temp = []\n",
        "        # Create generation-1, after evaluating the generation 0\n",
        "        MonteCarloList = []\n",
        "        for individual in self.population:\n",
        "            node = individual[GeneticAlgo._CHROMOSOME_INDEX]\n",
        "            for trials in range(10):\n",
        "                MonteCarloList.append(node.evaluate(GeneticAlgo._INPUTS,\n",
        "                                                    GeneticAlgo._TRUTH_VALUES,\n",
        "                                                    [\n",
        "                                                     random.choice([\n",
        "                                                                    True,\n",
        "                                                                    False])\n",
        "                                                     for _ in range(2)\n",
        "                                                     ] + [False])\n",
        "                                      )\n",
        "\n",
        "            _accuracy, _loss = np.mean(np.array(MonteCarloList), axis=0)\n",
        "\n",
        "            temp.append(\n",
        "                        [_accuracy, -np.absolute(_loss),\n",
        "                         individual[GeneticAlgo._TIE_BREAKING_INDEX],\n",
        "                         node]\n",
        "                        )\n",
        "\n",
        "        self.population = copy.deepcopy(temp)\n",
        "\n",
        "        # Sort the population based on the fitness, such that the fittest\n",
        "        # setup is first\n",
        "        self.population.sort(reverse=True)\n",
        "        del(temp)\n",
        "\n",
        "    def tournament_selection(self, participant_list):\n",
        "        \"\"\"tournament_selection Runs a tournament with the list of\n",
        "        participant tuples of Chromosomes and their metrics\n",
        "\n",
        "        :param participant_list: List of tuples of individuals\n",
        "        :type participant_list: List of tuples of Chromosomes and their metrics\n",
        "        :return: Tuple containing the 2 selected tuples of Chromosomes and\n",
        "        their metrics\n",
        "        :rtype: Tuple of tuples\n",
        "        \"\"\"\n",
        "        assert len(participant_list) == self.tournament_size, \\\n",
        "            f\"tournament_size {self.tournament_size} and participants_list \\\n",
        "                length: {len(participant_list)}, are not equal!\"\n",
        "        winner = None\n",
        "        # Run multiple round as long as the number of participants does not\n",
        "        # reduce to 2\n",
        "        while len(participant_list) != 2:\n",
        "            # print(len(participant_list))\n",
        "            winners_of_rounds = []\n",
        "            for battle_participants_ind in range(0, len(participant_list), 2):\n",
        "                # If there are 2 participants in a round\n",
        "                if battle_participants_ind <= len(participant_list) - 2:\n",
        "                    if participant_list[battle_participants_ind] > \\\n",
        "                       participant_list[battle_participants_ind + 1]:\n",
        "                        winner = participant_list[battle_participants_ind]\n",
        "                    else:\n",
        "                        winner = participant_list[battle_participants_ind + 1]\n",
        "\n",
        "                # If there is only participant in the round then that\n",
        "                # participant moves forward\n",
        "                elif battle_participants_ind == len(participant_list) - 1:\n",
        "                    winner = participant_list[battle_participants_ind]\n",
        "\n",
        "                assert winner is not None, \"Winner is None for some reason \\\n",
        "                and not a tuple of Chromosomes with metrics\"\n",
        "                winners_of_rounds.append(winner)\n",
        "\n",
        "            participant_list = winners_of_rounds\n",
        "            random.shuffle(participant_list)\n",
        "\n",
        "        return participant_list\n",
        "\n",
        "    def createNextGeneration(self, elitism_frac=0.1, tournament_size=20,\n",
        "                             mutation_frac=0.1):\n",
        "        self.tournament_size = tournament_size\n",
        "        newPopulation = []\n",
        "        # Order the parents based on the accuracy, loss and tie-breaking value\n",
        "        self.population.sort(reverse=True)\n",
        "\n",
        "        # Perform elitism, let the best parent go forward unchanged\n",
        "        for i in range(round(elitism_frac * self.population_count)):\n",
        "            temp = copy.deepcopy(self.population[i])\n",
        "            temp[GeneticAlgo._TIE_BREAKING_INDEX] = i\n",
        "            newPopulation.append(copy.deepcopy(temp))\n",
        "            del(temp)\n",
        "\n",
        "        \"\"\"\n",
        "        Tournament selection and Crossover have to be performed until the next\n",
        "        generation's population size is not equal self.population_count\n",
        "        \"\"\"\n",
        "        while len(newPopulation) != self.population_count:\n",
        "            # Select 2 parent from the initial population, and perfrom\n",
        "            # crossover, using tournament selection\n",
        "            # Select participants for the tournament\n",
        "            _participant_list = []\n",
        "            # A participant may appear multiple times in the tournament\n",
        "            _participant_indices = np.random.randint(0, self.population_count,\n",
        "                                                     tournament_size)\n",
        "            for _ in range(self.population_count):\n",
        "                # Check if the population member is a participant\n",
        "                if _ in _participant_indices:\n",
        "                    # Add the participant as many times as its index showed up\n",
        "                    # in the _participant_indices\n",
        "                    for _freq in range(np.\n",
        "                                       count_nonzero(_participant_indices == _)\n",
        "                                       ):\n",
        "                        _participant_list.append(self.population[_])\n",
        "                        # ..[np.copy(GeneticAlgo._CHROMOSOME_INDEX)])\n",
        "\n",
        "            assert len(_participant_list) == tournament_size,\\\n",
        "                f\"Participant list length {len(_participant_list)} \\\n",
        "                    != tournament_size {tournament_size}\"\n",
        "            # Shuffle the participants\n",
        "            random.shuffle(_participant_list)\n",
        "\n",
        "            # Run the tournament\n",
        "            _winner_1, _winner_2 = self\\\n",
        "                .tournament_selection(copy\n",
        "                                      .deepcopy(_participant_list))\n",
        "\n",
        "            # Perform cross-over on the 2 remaining winners at the end to\n",
        "            # get the child node\n",
        "            child_node = Chromosome\\\n",
        "                .cross_over(_winner_1[GeneticAlgo._CHROMOSOME_INDEX],\n",
        "                            _winner_2[GeneticAlgo._CHROMOSOME_INDEX])\n",
        "\n",
        "            # Find the accuracy and loss of the child node and append it to\n",
        "            # the new population\n",
        "            # Randomly enable or disbale dropouts in the different layers of\n",
        "            # the neural network - MonteCarlo Dropout\n",
        "            MonteCarloList = []\n",
        "            for i in range(10):\n",
        "                MonteCarloList.append(child_node\n",
        "                                      .evaluate(GeneticAlgo._INPUTS,\n",
        "                                                GeneticAlgo._TRUTH_VALUES,\n",
        "                                                [random.choice([True, False])\n",
        "                                                 for i in range(2)] + [False])\n",
        "                                      )\n",
        "\n",
        "            _accuracy, _loss = np.mean(np.array(MonteCarloList), axis=0)\n",
        "\n",
        "            newPopulation.append([_accuracy, -np.absolute(_loss), len(newPopulation),\n",
        "                                  child_node])\n",
        "\n",
        "        assert all([len(x) == 4 for x in newPopulation]) is True, \"The Length\\\n",
        "            of all members of newPopulation is not 4!\"\n",
        "\n",
        "        \"\"\"\n",
        "        Mutation has to be done after the new population has been created\n",
        "        \"\"\"\n",
        "        # Randomly select individuals without replacement from the new\n",
        "        # population to perform mutation on\n",
        "        rng = default_rng()\n",
        "        # mutation_candidates_index = np\\\n",
        "        #     .random\\\n",
        "        #     .shuffle(rng.choice(self.population_count,\n",
        "        #              size=round(self.population_count * mutation_frac),\n",
        "        #              replace=False))\n",
        "        mutation_candidates_index = rng.choice(self.population_count,\n",
        "                                               size=round(\n",
        "                                                          self.population_count * mutation_frac\n",
        "                                                          ),\n",
        "                                               replace=False)\n",
        "        np.random.shuffle(mutation_candidates_index)\n",
        "        assert mutation_candidates_index is not None, \"mutation_candidates_index is None!\"\n",
        "        _accuracy = None\n",
        "        _loss = None\n",
        "        for _ in range(self.population_count):\n",
        "            if _ in mutation_candidates_index:\n",
        "                newPopulation[_][GeneticAlgo._CHROMOSOME_INDEX].mutate(n=5)  # Change made here!\n",
        "\n",
        "                mutatedNode = newPopulation[_][GeneticAlgo._CHROMOSOME_INDEX]\n",
        "                # Recalculate the individual's accuracy and loss\n",
        "                MonteCarloList = []\n",
        "                for i in range(10):\n",
        "                    MonteCarloList.append(mutatedNode\n",
        "                                          .evaluate(GeneticAlgo._INPUTS,\n",
        "                                                    GeneticAlgo._TRUTH_VALUES,\n",
        "                                                    [random.choice([\n",
        "                                                                    True,\n",
        "                                                                    False\n",
        "                                                                    ])\n",
        "                                                     for i in range(2)]\n",
        "                                                    + [False])\n",
        "                                          )\n",
        "                    _accuracy, _loss = np.mean(np.array(MonteCarloList),\n",
        "                                               axis=0)\n",
        "                assert _accuracy is not None\n",
        "                assert _loss is not None\n",
        "                newPopulation[_] = [_accuracy, -np.absolute(_loss), newPopulation[_]\n",
        "                                    [GeneticAlgo._TIE_BREAKING_INDEX],\n",
        "                                    mutatedNode]\n",
        "\n",
        "        self.population = copy.deepcopy(newPopulation)\n",
        "        self.population.sort(reverse=True)\n",
        "\n",
        "    def runner(self, noOfGenerations):\n",
        "        for _ in range(noOfGenerations):\n",
        "            # self.createNextGeneration(mutation_frac=0.2, tournament_size=10)\n",
        "            self.createNextGeneration(mutation_frac=0.5, tournament_size=10)\n",
        "            if _ % 100 == 0:\n",
        "                print(f\"Iteration no.: {_}\")\n",
        "                print(\"The accuracy and loss of the best 5 individuals are:\")\n",
        "                for ind, individual in enumerate(self.population[:5]):\n",
        "                    print(f\"Individual no. : {ind}\")\n",
        "                    print(individual[GeneticAlgo._ACCURACY_INDEX])\n",
        "                    print(individual[GeneticAlgo._LOSS_INDEX])\n",
        "                    print(\"-\"*80)\n",
        "\n",
        "                _theEliteOne_layer_list = self.population[0][GeneticAlgo._CHROMOSOME_INDEX].getWeights()\n",
        "                temp = NeuralNet()\n",
        "                for ind, layer in enumerate(temp.layers):\n",
        "                    layer.set_params(_theEliteOne_layer_list[ind])\n",
        "\n",
        "                assert np.allclose(temp.layers[0].get_params(), _theEliteOne_layer_list[0]) \\\n",
        "                    is True, f\"{temp.layers[0].get_params()} != {_theEliteOne_layer_list[0]}\"\n",
        "                assert np.allclose(temp.layers[1].get_params(), _theEliteOne_layer_list[1]) \\\n",
        "                    is True, f\"{temp.layers[1].get_params()} != {_theEliteOne_layer_list[1]}\"\n",
        "                assert np.allclose(temp.layers[2].get_params(), _theEliteOne_layer_list[2]) \\\n",
        "                    is True, f\"{temp.layers[2].get_params()} != {_theEliteOne_layer_list[2]}\"\n",
        "\n",
        "\n",
        "                MonteCarloList = []\n",
        "                for i in range(10):\n",
        "                    # fit(self, inputs, _train, _numEpochs, truthValues):\n",
        "                    MonteCarloList.append(temp.fit(numpyinput_validation,\n",
        "                                                [random.choice([\n",
        "                                                                True,\n",
        "                                                                False\n",
        "                                                                ])\n",
        "                                                for i in range(2)]\n",
        "                                                    + [False], 1, numpyoutput_validation)\n",
        "                                            )\n",
        "                _accuracy, _loss = np.mean(np.array(sorted(MonteCarloList, reverse=True)[:5]),\n",
        "                                        axis=0)\n",
        "                print(\"The accuracy of the Elite one on VALIDATION is:\", _accuracy)\n",
        "                print(\"The loss of the Elite one on VALIDATION is:\", _loss)\n",
        "                \n",
        "                print(\"*\"*80, \"*\"*80, sep=\"\\n\")\n",
        "        \n",
        "        # Return the best Chromosome object\n",
        "        return self.population[0]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpbf4eGQYq6K"
      },
      "source": [
        "df = pd.read_csv(\"train_split.csv\")\n",
        "df.head()\n",
        "\n",
        "numpyinput = df[['Weight', 'HB', 'BP']].to_numpy()\n",
        "numpyinput = numpyinput.transpose()\n",
        "# numpyinput\n",
        "\n",
        "numpyoutput = df[['Result_0.0', 'Result_1.0']].to_numpy()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qh_RcWNYgmC"
      },
      "source": [
        "df_validation = pd.read_csv(\"valid_split.csv\")\n",
        "df_validation.head()\n",
        "\n",
        "numpyinput_validation = df[['Weight', 'HB', 'BP']].to_numpy()\n",
        "numpyinput_validation = numpyinput_validation.transpose()\n",
        "# numpyinput\n",
        "\n",
        "numpyoutput_validation = df[['Result_0.0', 'Result_1.0']].to_numpy()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LeyWDMR5tHD",
        "outputId": "af8eefa9-4b23-4d53-d646-e08d3fd73080",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "darwin = GeneticAlgo(inputs=numpyinput, truthValues=numpyoutput)\n",
        "theEliteOne = darwin.runner(1000)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration no.: 0\n",
            "The accuracy and loss of the best 5 individuals are:\n",
            "Individual no. : 0\n",
            "0.7861538461538461\n",
            "-0.3101657058427929\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 1\n",
            "0.7846153846153846\n",
            "-0.2672524095414535\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 2\n",
            "0.7846153846153846\n",
            "-0.28366085531731133\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 3\n",
            "0.7846153846153846\n",
            "-0.2868819896861228\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 4\n",
            "0.7846153846153846\n",
            "-0.2910792112845505\n",
            "--------------------------------------------------------------------------------\n",
            "The accuracy of the Elite one on VALIDATION is: 0.7876923076923077\n",
            "The loss of the Elite one on VALIDATION is: 0.31677661453560474\n",
            "********************************************************************************\n",
            "********************************************************************************\n",
            "Iteration no.: 100\n",
            "The accuracy and loss of the best 5 individuals are:\n",
            "Individual no. : 0\n",
            "0.8923076923076925\n",
            "-0.166912418473313\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 1\n",
            "0.8923076923076925\n",
            "-0.1719534292108461\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 2\n",
            "0.8923076923076925\n",
            "-0.17323483117163235\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 3\n",
            "0.8923076923076925\n",
            "-0.1754442275938802\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 4\n",
            "0.8923076923076925\n",
            "-0.1769033962087995\n",
            "--------------------------------------------------------------------------------\n",
            "The accuracy of the Elite one on VALIDATION is: 0.8923076923076924\n",
            "The loss of the Elite one on VALIDATION is: 0.18277964082237771\n",
            "********************************************************************************\n",
            "********************************************************************************\n",
            "Iteration no.: 200\n",
            "The accuracy and loss of the best 5 individuals are:\n",
            "Individual no. : 0\n",
            "0.8923076923076925\n",
            "-0.16527607743666345\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 1\n",
            "0.8923076923076925\n",
            "-0.1672915719405706\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 2\n",
            "0.8923076923076925\n",
            "-0.1673610528073952\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 3\n",
            "0.8923076923076925\n",
            "-0.16801972853622354\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 4\n",
            "0.8923076923076925\n",
            "-0.16947526500479457\n",
            "--------------------------------------------------------------------------------\n",
            "The accuracy of the Elite one on VALIDATION is: 0.8923076923076924\n",
            "The loss of the Elite one on VALIDATION is: 0.1692101752875181\n",
            "********************************************************************************\n",
            "********************************************************************************\n",
            "Iteration no.: 300\n",
            "The accuracy and loss of the best 5 individuals are:\n",
            "Individual no. : 0\n",
            "0.8923076923076925\n",
            "-0.1595670993206264\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 1\n",
            "0.8923076923076925\n",
            "-0.16205988366576943\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 2\n",
            "0.8923076923076925\n",
            "-0.16355497976365072\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 3\n",
            "0.8923076923076925\n",
            "-0.16358253328324204\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 4\n",
            "0.8923076923076925\n",
            "-0.16532646354154146\n",
            "--------------------------------------------------------------------------------\n",
            "The accuracy of the Elite one on VALIDATION is: 0.8923076923076924\n",
            "The loss of the Elite one on VALIDATION is: 0.16462090195311577\n",
            "********************************************************************************\n",
            "********************************************************************************\n",
            "Iteration no.: 400\n",
            "The accuracy and loss of the best 5 individuals are:\n",
            "Individual no. : 0\n",
            "0.8923076923076925\n",
            "-0.16178254840154777\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 1\n",
            "0.8923076923076925\n",
            "-0.16616206690783425\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 2\n",
            "0.8923076923076925\n",
            "-0.16668539510952837\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 3\n",
            "0.8923076923076925\n",
            "-0.16837819951713856\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 4\n",
            "0.8923076923076925\n",
            "-0.17438037008476695\n",
            "--------------------------------------------------------------------------------\n",
            "The accuracy of the Elite one on VALIDATION is: 0.8923076923076924\n",
            "The loss of the Elite one on VALIDATION is: 0.16987081388405062\n",
            "********************************************************************************\n",
            "********************************************************************************\n",
            "Iteration no.: 500\n",
            "The accuracy and loss of the best 5 individuals are:\n",
            "Individual no. : 0\n",
            "0.9061538461538463\n",
            "-0.16171756681325697\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 1\n",
            "0.9015384615384617\n",
            "-0.16063080358953274\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 2\n",
            "0.9015384615384617\n",
            "-0.16418432165958313\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 3\n",
            "0.9000000000000001\n",
            "-0.16300110795964856\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 4\n",
            "0.8984615384615386\n",
            "-0.1604048018068431\n",
            "--------------------------------------------------------------------------------\n",
            "The accuracy of the Elite one on VALIDATION is: 0.9015384615384615\n",
            "The loss of the Elite one on VALIDATION is: 0.16134771305053655\n",
            "********************************************************************************\n",
            "********************************************************************************\n",
            "Iteration no.: 600\n",
            "The accuracy and loss of the best 5 individuals are:\n",
            "Individual no. : 0\n",
            "0.9046153846153848\n",
            "-0.18862190313253746\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 1\n",
            "0.9030769230769232\n",
            "-0.17697955285615513\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 2\n",
            "0.9030769230769232\n",
            "-0.19520108782502324\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 3\n",
            "0.9030769230769232\n",
            "-0.1970042661015375\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 4\n",
            "0.9015384615384617\n",
            "-0.1834203756541239\n",
            "--------------------------------------------------------------------------------\n",
            "The accuracy of the Elite one on VALIDATION is: 0.9046153846153846\n",
            "The loss of the Elite one on VALIDATION is: 0.1922777144726072\n",
            "********************************************************************************\n",
            "********************************************************************************\n",
            "Iteration no.: 700\n",
            "The accuracy and loss of the best 5 individuals are:\n",
            "Individual no. : 0\n",
            "0.9030769230769232\n",
            "-0.1727370789385725\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 1\n",
            "0.9030769230769232\n",
            "-0.17986670965757748\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 2\n",
            "0.9030769230769232\n",
            "-0.18165327961530853\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 3\n",
            "0.9015384615384617\n",
            "-0.18536334849788\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 4\n",
            "0.9015384615384617\n",
            "-0.19833390575354062\n",
            "--------------------------------------------------------------------------------\n",
            "The accuracy of the Elite one on VALIDATION is: 0.9046153846153846\n",
            "The loss of the Elite one on VALIDATION is: 0.17183465325199104\n",
            "********************************************************************************\n",
            "********************************************************************************\n",
            "Iteration no.: 800\n",
            "The accuracy and loss of the best 5 individuals are:\n",
            "Individual no. : 0\n",
            "0.9015384615384617\n",
            "-0.1843903712471714\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 1\n",
            "0.9015384615384617\n",
            "-0.18886396847246528\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 2\n",
            "0.9000000000000001\n",
            "-0.18725868289322475\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 3\n",
            "0.9000000000000001\n",
            "-0.18755215158084895\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 4\n",
            "0.8984615384615386\n",
            "-0.18260324881855658\n",
            "--------------------------------------------------------------------------------\n",
            "The accuracy of the Elite one on VALIDATION is: 0.9046153846153846\n",
            "The loss of the Elite one on VALIDATION is: 0.18968814759465938\n",
            "********************************************************************************\n",
            "********************************************************************************\n",
            "Iteration no.: 900\n",
            "The accuracy and loss of the best 5 individuals are:\n",
            "Individual no. : 0\n",
            "0.9061538461538463\n",
            "-0.1880039128210746\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 1\n",
            "0.9061538461538463\n",
            "-0.19161308579608055\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 2\n",
            "0.9046153846153848\n",
            "-0.18991613155906348\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 3\n",
            "0.9046153846153848\n",
            "-0.19161027211460996\n",
            "--------------------------------------------------------------------------------\n",
            "Individual no. : 4\n",
            "0.9030769230769232\n",
            "-0.1973195133774581\n",
            "--------------------------------------------------------------------------------\n",
            "The accuracy of the Elite one on VALIDATION is: 0.9015384615384615\n",
            "The loss of the Elite one on VALIDATION is: 0.20196539217441192\n",
            "********************************************************************************\n",
            "********************************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlTqGb4m5puz"
      },
      "source": [
        "df_test = pd.read_csv(\"test_split.csv\")\n",
        "df_test.head()\n",
        "\n",
        "numpyinput_test = df[['Weight', 'HB', 'BP']].to_numpy()\n",
        "numpyinput_test = numpyinput_test.transpose()\n",
        "# numpyinput\n",
        "\n",
        "numpyoutput_test = df[['Result_0.0', 'Result_1.0']].to_numpy()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6DY49K0YrDS",
        "outputId": "00d5b737-d9e1-4f74-89ee-f357757cc258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"The accuracy of the Elite one on TRAIN is:\", theEliteOne[GeneticAlgo._ACCURACY_INDEX])\n",
        "print(\"The loss of the Elite one ON TRAIN is\", theEliteOne[GeneticAlgo._LOSS_INDEX])\n",
        "\n",
        "# theEliteOne[GeneticAlgo._CHROMOSOME_INDEX].neural_net_obj\n",
        "theEliteOne_layer_list = theEliteOne[GeneticAlgo._CHROMOSOME_INDEX].getWeights()\n",
        "temp = NeuralNet()\n",
        "for ind, layer in enumerate(temp.layers):\n",
        "    layer.set_params(theEliteOne_layer_list[ind])\n",
        "\n",
        "assert np.allclose(temp.layers[0].get_params(), theEliteOne_layer_list[0]) \\\n",
        "    is True, f\"{temp.layers[0].get_params()} != {theEliteOne_layer_list[0]}\"\n",
        "assert np.allclose(temp.layers[1].get_params(), theEliteOne_layer_list[1]) \\\n",
        "    is True, f\"{temp.layers[1].get_params()} != {theEliteOne_layer_list[1]}\"\n",
        "assert np.allclose(temp.layers[2].get_params(), theEliteOne_layer_list[2]) \\\n",
        "    is True, f\"{temp.layers[2].get_params()} != {theEliteOne_layer_list[2]}\"\n",
        "\n",
        "\n",
        "MonteCarloList = []\n",
        "for i in range(10):\n",
        "    # fit(self, inputs, _train, _numEpochs, truthValues):\n",
        "    MonteCarloList.append(temp.fit(numpyinput_test,\n",
        "                                 [random.choice([\n",
        "                                                 True,\n",
        "                                                 False\n",
        "                                                 ])\n",
        "                                  for i in range(2)]\n",
        "                                    + [False], 1, numpyoutput_test)\n",
        "                            )\n",
        "_accuracy, _loss = np.mean(np.array(sorted(MonteCarloList, reverse=True)[:5]),\n",
        "                           axis=0)\n",
        "print(\"The accuracy of the Elite one on TEST is:\", _accuracy)\n",
        "print(\"The loss of the Elite one on TEST is:\", _loss)\n",
        "\n",
        "# theEliteOne[GeneticAlgo._CHROMOSOME_INDEX].neural_net_obj().fit()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of the Elite one on TRAIN is: 0.9061538461538463\n",
            "The loss of the Elite one ON TRAIN is -0.17881959411451823\n",
            "The accuracy of the Elite one on TEST is: 0.9046153846153846\n",
            "The loss of the Elite one on TEST is: 0.18423532463511444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE-O7D0JYucA",
        "outputId": "2a845538-ccda-4350-b0f5-5a4035ddd390",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.mean(np.array(MonteCarloList), axis=0)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.88615385, 0.18635385])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U-jJs9DYuW6"
      },
      "source": [
        "# numpyoutput = numpyoutput.transpose()\n",
        "# numpyoutput\n",
        "\n",
        "# network = NeuralNet()\n",
        "# network.fit(numpyinput, [True, True, False], 1, numpyoutput)\n",
        "\n",
        "# -sum([np.log(0.48958263+10e-8), 0])/2, -np.log(0.48958263+10e-8)/2\n",
        "\n",
        "# a = [[1, 2, 3],[3, 4, 5],[6, 7, 8]]\n",
        "# a = np.array(a).transpose()\n",
        "# Layer.softmax_Prime(a)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvYwQKXEbf4Q",
        "outputId": "3104fedb-8cb4-4419-8a03-fca879964a81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "MonteCarloList"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.8923076923076924, 0.17139681387530994),\n",
              " (0.7846153846153846, 0.18531031495072064),\n",
              " (0.9076923076923077, 0.17817079090051935),\n",
              " (0.8923076923076924, 0.1948299758092458),\n",
              " (0.8769230769230769, 0.20256210159963411),\n",
              " (0.9076923076923077, 0.17817079090051935),\n",
              " (0.9076923076923077, 0.17817079090051935),\n",
              " (0.8923076923076924, 0.20849345957349472),\n",
              " (0.9076923076923077, 0.17817079090051935),\n",
              " (0.8923076923076924, 0.18826262775419994)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6ZKKsK8bgsd"
      },
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "pickle.dump( theEliteOne, open( \"theEliteOne.p\", \"wb\" ) )\n",
        "\n",
        "theEliteOne = pickle.load( open( \"theEliteOne.p\", \"rb\" ) )"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAf3w7nUf-eN",
        "outputId": "5f61df97-28e8-4287-ce11-11a3a295db16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# theEliteOne[GeneticAlgo._CHROMOSOME_INDEX].neural_net_obj\n",
        "theEliteOne_layer_list = theEliteOne[GeneticAlgo._CHROMOSOME_INDEX].getWeights()\n",
        "temp = NeuralNet()\n",
        "for ind, layer in enumerate(temp.layers):\n",
        "    layer.set_params(theEliteOne_layer_list[ind])\n",
        "\n",
        "assert np.allclose(temp.layers[0].get_params(), theEliteOne_layer_list[0]) \\\n",
        "    is True, f\"{temp.layers[0].get_params()} != {theEliteOne_layer_list[0]}\"\n",
        "assert np.allclose(temp.layers[1].get_params(), theEliteOne_layer_list[1]) \\\n",
        "    is True, f\"{temp.layers[1].get_params()} != {theEliteOne_layer_list[1]}\"\n",
        "assert np.allclose(temp.layers[2].get_params(), theEliteOne_layer_list[2]) \\\n",
        "    is True, f\"{temp.layers[2].get_params()} != {theEliteOne_layer_list[2]}\"\n",
        "\n",
        "\n",
        "MonteCarloList = []\n",
        "for i in range(10):\n",
        "    # fit(self, inputs, _train, _numEpochs, truthValues):\n",
        "    MonteCarloList.append(temp.fit(numpyinput_test,\n",
        "                                 [random.choice([\n",
        "                                                 True,\n",
        "                                                 False\n",
        "                                                 ])\n",
        "                                  for i in range(2)]\n",
        "                                    + [False], 1, numpyoutput_test)\n",
        "                            )\n",
        "_accuracy, _loss = np.mean(np.array(sorted(MonteCarloList, reverse=True)[:5]),\n",
        "                           axis=0)\n",
        "print(\"The accuracy of the Elite one on TEST is:\", _accuracy)\n",
        "print(\"The loss of the Elite one on TEST is:\", _loss)\n",
        "\n",
        "# theEliteOne[GeneticAlgo._CHROMOSOME_INDEX].neural_net_obj().fit()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of the Elite one on TEST is: 0.9076923076923077\n",
            "The loss of the Elite one on TEST is: 0.17817079090051935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbxErdXPgCgx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}